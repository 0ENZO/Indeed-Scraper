{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Indeed_Scraper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jFnOQUpO1X8"
      },
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFLT0tBuNwcM",
        "outputId": "da6ef0c5-b9cc-483b-f8aa-2cdd20557c07"
      },
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import requests\n",
        "import time\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from random import randint\n",
        "from time import sleep\n",
        "import base64\n",
        "\n",
        "def get_soup(url):\n",
        "    USER_AGENTS = [\n",
        "      ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'),\n",
        "      ('Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.110 Safari/537.36'),  # chrome\n",
        "      ('Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.79 Safari/537.36'),  # chrome\n",
        "      ('Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) Gecko/20100101 Firefox/55.0')  # firefox\n",
        "    ]\n",
        "    ind = random.randint(0, len(USER_AGENTS) - 1)\n",
        "    headers = {'User-Agent': USER_AGENTS[ind]}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout = 10)\n",
        "        soup = BeautifulSoup(response.text)\n",
        "    except:\n",
        "        print(\"Impossible de se connecter\", response.status_code)\n",
        "    return soup\n",
        "\n",
        "\n",
        "def scrape_offers(offer_title, offer_location, offer_type, offer_date):\n",
        "    more_links = True\n",
        "    page = 1\n",
        "    offers = []\n",
        "    offer_url = f'https://api.scraperapi.com?api_key={tokens[\"scraperapi\"]}&url=https://www.indeed.fr/voir-emploi?jk='\n",
        "    #if offer_title == 'all':\n",
        "    #  url = f'https://api.scraperapi.com?api_key={tokens[\"scraperapi\"]}&url=https://fr.indeed.com/emplois?q={offer_title}&l={offer_location}&fromage={offer_date}&render=true'\n",
        "    #else:\n",
        "    api_flags = \"&render=true&keep_headers=true\"\n",
        "    url = f'https://api.scraperapi.com?api_key={tokens[\"scraperapi\"]}&url=https://fr.indeed.com/emplois?q={offer_title}&l={offer_location}&jt={offer_type}&fromage={offer_date}{api_flags}'\n",
        "\n",
        "    USER_AGENTS = [\n",
        "      ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'),\n",
        "      ('Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.110 Safari/537.36'),  # chrome\n",
        "      ('Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.79 Safari/537.36'),  # chrome\n",
        "      ('Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) Gecko/20100101 Firefox/55.0')  # firefox\n",
        "    ]\n",
        "    ind = random.randint(0, len(USER_AGENTS) - 1)\n",
        "    headers = {'User-Agent': USER_AGENTS[ind]}\n",
        "\n",
        "    while(more_links):\n",
        "        response = requests.get(url, headers=headers, timeout = 100)\n",
        "        soup = BeautifulSoup(response.text)\n",
        "\n",
        "        for item in soup.select('a.tapItem'):\n",
        "            offer = {}\n",
        "\n",
        "            #On récupère le dernier element pour éviter de récupérer le tag \"nouveau\"\n",
        "            offer['title'] = item.select_one('h2.jobTitle').find_all(\"span\")[-1].text.strip()\n",
        "\n",
        "            offer['link'] = item['data-jk']\n",
        "\n",
        "            try:\n",
        "              offer['date'] = item.select_one('span.date').text.strip()\n",
        "            except: \n",
        "              offer['date'] = 'None'\n",
        "\n",
        "            try:\n",
        "              offer['company'] = item.select_one('span.companyName').text.strip()\n",
        "            except: \n",
        "              offer['company'] = 'None'\n",
        "\n",
        "            try:\n",
        "              offer['location'] = item.select_one('div.companyLocation').text\n",
        "            except: \n",
        "              offer['location'] = 'None'\n",
        "\n",
        "            try:\n",
        "              offer['summary'] = item.select_one('div.job-snippet').text.strip()\n",
        "            except: \n",
        "              offer['summary'] = 'None'\n",
        "            \n",
        "            try:\n",
        "              offer['rate'] = item.select_one('span.ratingNumber').text.strip()\n",
        "            except: \n",
        "              offer['rate'] = 'None'\n",
        "\n",
        "            #detailedResponse = requests.get(offer_url+offer['link'], headers=headers, timeout = 100)\n",
        "            #detailedSoup = BeautifulSoup(detailedResponse.text, 'html.parser')\n",
        "            #\n",
        "            #offer['description'] = []\n",
        "            #for tag in detailedSoup.select('div.jobsearch-jobDescriptionText'):\n",
        "            #  try:\n",
        "            #      offer['description'].append([''.join(tag.text.strip())])\n",
        "            #  except AttributeError:\n",
        "            #      offer['description'].append([''])\n",
        "\n",
        "            offers.append(offer)\n",
        "            sleep(randint(1,8))\n",
        "\n",
        "        next_link = soup.findAll(\"a\", {\"aria-label\" : \"Suivant\"})\n",
        "        if(next_link):\n",
        "            page += 1\n",
        "            url = f\"https://api.scraperapi.com?api_key={tokens['scraperapi']}&url=https://fr.indeed.com/\"+next_link[0].get('href')+api_flags\n",
        "        else:\n",
        "            more_links = False\n",
        "\n",
        "    print(f'{page} pages scrappé(s)')\n",
        "    return offers\n",
        "\n",
        "def main():\n",
        "  st.set_page_config(page_title='Indeed Scrapper')\n",
        "  st.title(\"Indeed Scraper\")\n",
        "  st.subheader(\"Faites votre recherche\")\n",
        "  offer_title_st = st.sidebar.text_input(\"Poste\")\n",
        "  offer_types = ['Temps plein','CDI','CDD','Apprentissage','Contrat pro', 'Intérim', 'Freelance / Indépendant']\n",
        "  offer_type_st = st.sidebar.selectbox('Type de contrat',offer_types)\n",
        "\n",
        "  #if offer_type_st == \"Tous\":\n",
        "  #  offer_type = \"all\"\n",
        "  if offer_type_st == \"Temps plein\":\n",
        "    offer_type = \"fulltime\"\n",
        "  elif offer_type_st == \"CDI\":\n",
        "    offer_type = \"permanent\"\n",
        "  elif offer_type_st == \"CDD\":\n",
        "    offer_type = \"contract\"\n",
        "  elif offer_type_st == \"Apprentissage\":\n",
        "    offer_type = \"apprenticeship\" \n",
        "  elif offer_type_st == \"Contrat pro\":\n",
        "    offer_type = \"custom_1\"\n",
        "  elif offer_type_st == \"Intérim\":\n",
        "    offer_type = \"temporary\"\n",
        "  elif offer_type_st == \"Freelance / Indépendant\":\n",
        "    offer_type = \"subcontract\"\n",
        "  \n",
        "  offer_location_st = st.sidebar.text_input(\"Localisation\")\n",
        "  offer_dates = ['Tout','Denières 24 heures','3 derniers jours','7 derniers jours','14 derniers jours']\n",
        "  offer_date_st = st.sidebar.selectbox(\"Date de publication\",offer_dates)\n",
        "\n",
        "  if offer_date_st == 'Tout':\n",
        "      offer_date = \"last\"\n",
        "  elif offer_date_st == 'Denières 24 heures':\n",
        "      offer_date = 1\n",
        "  elif offer_date_st == '3 derniers jours':\n",
        "      offer_date = 3\n",
        "  elif offer_date_st == '7 derniers jours':\n",
        "      offer_date = 7\n",
        "  elif offer_date_st == '14 derniers jours':\n",
        "      offer_date = 14\n",
        "\n",
        "  search_st = st.sidebar.button(\"Rechercher\")\n",
        "  if search_st:\n",
        "    offer_title = offer_title_st.replace(\" \",\"+\")\n",
        "    offer_location = offer_location_st.replace(\" \", \"+\")\n",
        "    #st.markdown(f'https://api.scraperapi.com?api_key={tokens[\"scraperapi\"]}&url=https://fr.indeed.com/emplois?q={offer_title}&l={offer_location}&jt={offer_type}&fromage={offer_date}&render=true')\n",
        "    with st.spinner('En cours de récupération...'):\n",
        "      offers = scrape_offers(offer_title, offer_location, offer_type, offer_date)\n",
        "      df = pd.DataFrame(offers)\n",
        "      list_of_col_names = ['title', 'company', 'location', 'summary', 'date', 'rate', 'description', 'link']\n",
        "      df = df.filter(list_of_col_names)\n",
        "      df.to_csv('offers.csv')\n",
        "\n",
        "      csv = df.to_csv(index=False)\n",
        "      b64 = base64.b64encode(csv.encode()).decode()\n",
        "      href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"offers.csv\">Télécharger au format csv</a>'\n",
        "\n",
        "    st.success('Terminé !')\n",
        "    st.markdown(href, unsafe_allow_html=True)\n",
        "    st.dataframe(df)\n",
        "  \n",
        "if __name__ == '__main__':\n",
        "  tokens = {}\n",
        "  with open('pwd.txt') as f:\n",
        "    for line in f:\n",
        "        x = line.split(\":\")\n",
        "        tokens[x[0]] = x[1].rstrip(\"\\n\")\n",
        "\n",
        "  main()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU8OvZkoO5mT",
        "outputId": "00e885db-eb0d-423b-d96b-ce5755b5aaa3"
      },
      "source": [
        "!ngrok authtoken tokens['ngrok']"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaGR1ZmKN3Kc"
      },
      "source": [
        "#!streamlit run app.py &>/dev/null&\n",
        "!streamlit run --server.port 80 app.py &>/dev/null& "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgak-nnPO7pa"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(port='8501')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5tp-CLuO_Fa",
        "outputId": "d1d596ab-de78-43d6-d552-35f448d330f3"
      },
      "source": [
        "public_url"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://7f4dc5db6379.ngrok.io\" -> \"http://localhost:80\">"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoQ7_Q3oQ8bm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a68ca1-438f-4eaa-c332-e5c8e2267db5"
      },
      "source": [
        "!pgrep streamlit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6nNpocvPEld"
      },
      "source": [
        "!kill 154\n",
        "ngrok.kill()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqMDaolMwULS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}